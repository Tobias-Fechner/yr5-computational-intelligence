{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimising Character Classification Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab3.nn_general import NeuralNetwork\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 13})\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchTrain(data_training, data_validation, nn, batchSize, epochs):\n",
    "    \n",
    "    assert len(data_training)% batchSize == 0\n",
    "    \n",
    "    trainingCurve = []\n",
    "    validationCurve = []\n",
    "    \n",
    "    for i in range(epochs):\n",
    "\n",
    "        batchStart = 0\n",
    "        batchEnd = batchSize - 1\n",
    "        \n",
    "        print(\"epoch number: \", i)\n",
    "        \n",
    "        while batchEnd < len(data_training)-1:\n",
    "\n",
    "            data_batch = data_training[batchStart:batchEnd]\n",
    "\n",
    "            nn = train(data_batch, nn)\n",
    "\n",
    "            batchstart = batchEnd + 1\n",
    "            batchEnd += batchSize\n",
    "        \n",
    "        trainingLearning = test(data_training, nn)\n",
    "        validationLearning = test(data_validation, nn)\n",
    "        \n",
    "        trainingCurve.append(trainingLearning)\n",
    "        validationCurve.append(validationLearning)\n",
    "    \n",
    "    return nn, trainingCurve, validationCurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, nn):\n",
    "    # Train the neural network on each trainingsample\n",
    "    for record in data:\n",
    "\n",
    "        # Split the record by the commas\n",
    "        pixelValues = record.split(',')\n",
    "        label = pixelValues.pop(0)\n",
    "\n",
    "        # Scale and shift the inputs from 0..255 to 0.01..1\n",
    "        inputs = (np.asfarray(pixelValues) / 255.0 * 0.99) + 0.01\n",
    "\n",
    "        # Create the target output values (all 0.01, except the desired label which is 0.99)\n",
    "        targets = np.zeros(nn.output_nodes) + 0.01\n",
    "\n",
    "        # pixelValues[0] is the target label for this record\n",
    "        targets[int(label)] = 0.99\n",
    "\n",
    "        # Train the network\n",
    "        nn.train(inputs.tolist(), targets.tolist())\n",
    "        \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data, nn):\n",
    "    \n",
    "    scorecard = []\n",
    "    successRate = 0\n",
    "    \n",
    "    for record in data:\n",
    "        # Split the record by commas\n",
    "        pixelValues = record.split(',')\n",
    "\n",
    "        # Correct label is the first value\n",
    "        correct_label = int(pixelValues.pop(0))\n",
    "\n",
    "        # Scale and shift the inputs\n",
    "        inputs = (np.asfarray(pixelValues)/255.0*0.99) + 0.01\n",
    "\n",
    "        # Query the network\n",
    "        outputs = nn.queryNN(inputs.tolist())\n",
    "\n",
    "        # Identify predicted label\n",
    "        prediction = np.argmax(outputs)\n",
    "\n",
    "        # Add to scorecard\n",
    "        if prediction == correct_label:\n",
    "            scorecard.append(1)\n",
    "        else:\n",
    "            scorecard.append(0)\n",
    "            \n",
    "    scorecard = np.asarray(scorecard)\n",
    "    successRate = (scorecard.sum()/scorecard.size)*100\n",
    "    return successRate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load len(500) Training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"datasources\\mnist_train_500.csv\", 'r')\n",
    "data_training = file.readlines()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"datasources\\mnist_test_100.csv\", 'r')\n",
    "data_testing = file.readlines()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify nn parameters to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hidden_nodes = [50, 75, 100, 200, 300, 500, 700, 900, 1100]\n",
    "lrs = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new dict to store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate Hidden Nodes and LR Over Train/ Test Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each number of hidden nodes\n",
    "for hidden_nodes in tqdm(x_hidden_nodes):\n",
    "    data = {}\n",
    "    \n",
    "    # Loop through each learning rate\n",
    "    for lr in lrs:\n",
    "        # Create new nn\n",
    "        nn = NeuralNetwork(input_nodes=784, \n",
    "                           hidden_nodes=hidden_nodes, \n",
    "                           output_nodes=10, \n",
    "                           lr=lr)\n",
    "        \n",
    "        # Train that nn 10 times (epochs)\n",
    "        for i in range(10):\n",
    "            nn = train(data_training, nn)\n",
    "            \n",
    "        # Test that nn, return also successRate\n",
    "        successRate = test(data_testing, nn)\n",
    "        \n",
    "        # Store success rate *for that lr* in a dict\n",
    "        data[lr] = successRate\n",
    "        \n",
    "    # Store in dict containing success scores for each lr *for that number of hidden nodes* \n",
    "    results[str(hidden_nodes)] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[7, 7], dpi=100)\n",
    "ax1 = fig.add_subplot()\n",
    "\n",
    "ax1.set_xlabel('learning rate')\n",
    "ax1.set_ylabel('success rate, %')\n",
    "ax1.set_title('Network Performance with Learning Rate \\nand Number of Nodes in Hidden Layer')\n",
    "\n",
    "for i in results.keys():\n",
    "    x = results[i].keys()\n",
    "    y = results[i].values()\n",
    "    ax1.scatter(x, y, label=i, s=int(i))\n",
    "\n",
    "ax1.legend(loc='center left', bbox_to_anchor=(1, 0.5), title='Hidden Nodes')\n",
    "ax1.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in results.keys():\n",
    "    bestResult = max(results[i], key=lambda y: abs(results[i][y]))\n",
    "    print(i, bestResult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Best Performers for Further Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create df because dataframes are nice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_performance = pd.DataFrame(results)\n",
    "data_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select best 3 number of hidden nodes for each LR and sort by performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = pd.DataFrame()\n",
    "\n",
    "for col in data_performance.columns:\n",
    "    # Get and sort column values as series\n",
    "    values = data_performance[col].sort_values(ascending=False)\n",
    "    # Select top three performers\n",
    "    tops = vals.iloc[:3]\n",
    "    \n",
    "    # Form into suitable df and append\n",
    "    df = pd.DataFrame(tops)\n",
    "    df.reset_index(level=0, inplace=True)\n",
    "    df.columns = ['learningRates','performance']\n",
    "    df['hiddenNodes'] = col\n",
    "    selected = selected.append(df, ignore_index=True)\n",
    "    \n",
    "selected.sort_values(by='performance', ascending=False, inplace=True, ignore_index=True)\n",
    "selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain with Learning Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get training data with 5000 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"datasources\\mnist_train_5000.csv\", 'r')\n",
    "data_training = file.readlines()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get validation data of 500 lines from other end of full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"datasources\\mnist_validation_500.csv\", 'r')\n",
    "data_validation = file.readlines()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get test data of 100 lines from completely independent dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"datasources\\mnist_test_100.csv\", 'r')\n",
    "data_testing = file.readlines()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load 10 best performers from excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = pd.read_excel('.\\datasources\\selected.xlsx', index_col=0)\n",
    "selected = selected.sort_values(by='performanceUpdated',ascending=False, ignore_index=True).head(3)\n",
    "selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=20\n",
    "batchSize=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in tqdm(selected.iterrows()):\n",
    "    \n",
    "    # Create new nn\n",
    "    nn = NeuralNetwork(input_nodes=784, \n",
    "                       hidden_nodes=int(row['hiddenNodes']), \n",
    "                       output_nodes=10, \n",
    "                       lr=float(row['learningRates']))\n",
    "\n",
    "    nn, trainingCurve, validationCurve = batchTrain(data_training=data_training, \n",
    "                                                    data_validation=data_validation, \n",
    "                                                    nn=nn, \n",
    "                                                    batchSize=batchSize, \n",
    "                                                    epochs=epochs)\n",
    "\n",
    "    plt.plot(range(1, epochs+1), trainingCurve, label='trainingCurve')\n",
    "    plt.plot(range(1, epochs+1), validationCurve, label='validationCurve')\n",
    "    plt.ylabel('performance')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.show()\n",
    "    \n",
    "    selected.at[index, 'trainingCurve'] = \",\".join([str(x) for x in trainingCurve])\n",
    "    selected.at[index, 'validationCurve'] = \",\".join([str(x) for x in validationCurve])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected['performance5k'] = selected['validationCurve'].apply(lambda x: max(x.split(',')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected.to_csv('.\\datasources\\selected_5k.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
