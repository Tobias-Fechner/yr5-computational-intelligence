{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nn_spikes import NeuralNetwork, batchTrain\n",
    "from spike_tools import classifySpikesMLP, getSpikeWaveforms\n",
    "import plotly.express as px\n",
    "from simulated_annealing import anneal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./datasources/spikes/training_data.csv')\n",
    "spikeLocations = pd.read_csv('./datasources/spikes/training_spike_locations.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_training = pd.read_csv('./datasources/spikes/dev/data_training_SA.csv')\n",
    "data_training.set_index(data_training.columns[0], drop=True, inplace=True)\n",
    "data_training.index.name='index'\n",
    "data_training.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_validation = pd.read_csv('./datasources/spikes/dev/data_validation_SA.csv')\n",
    "data_validation.set_index(data_validation.columns[0], drop=True, inplace=True)\n",
    "data_validation.index.name='index'\n",
    "data_validation.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spikeIndexes_training = pd.read_csv('./datasources/spikes/dev/spikeIndexes_training_SA.csv')\n",
    "spikeIndexes_training.set_index(spikeIndexes_training.columns[0], drop=True, inplace=True)\n",
    "spikeIndexes_training = spikeIndexes_training.values.flatten()\n",
    "spikeIndexes_training[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spikeIndexes_validation = pd.read_csv('./datasources/spikes/dev/spikeIndexes_validation_SA.csv')\n",
    "spikeIndexes_validation.set_index(spikeIndexes_validation.columns[0], drop=True, inplace=True)\n",
    "spikeIndexes_validation = spikeIndexes_validation.values.flatten()\n",
    "spikeIndexes_validation[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run simulated annealing optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs, hidden_nodes, lr\n",
    "solution = [15,500,0.2] \n",
    "\n",
    "# Simulated annealing optimisation\n",
    "final_solution, finalError, cost_values = anneal(solution, spikeLocations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveforms = data_validation.loc[spikeIndexes_validation, 'waveform']\n",
    "predictions = spike_tools.classifySpikesMLP(waveforms, results['1100']['nn'])\n",
    "data_validation.at[spikeIndexes_validation, 'predictedClass'] = pd.Series(predictions).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_validation.loc[spikeIndexes_validation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataPreProcess(df, spikeLocations, threshold=0.85, submission=False, detectPeaksOn='signalSavgolBP', waveformWindow=60, waveformSignalType='signalSavgol'):\n",
    "\n",
    "    data = df\n",
    "\n",
    "    # if not 'signalSavgol' in data.columns:\n",
    "    data['signalSavgol'] = savgol_filter(data['signal'], 17, 2)\n",
    "    data['signalSavgolBP'] = bandPassFilter(data['signalSavgol'])\n",
    "    data, predictedSpikeIndexes = detectPeaks(data, detectPeaksOn=detectPeaksOn, threshold=threshold)\n",
    "    # else:\n",
    "    #     predictedSpikeIndexes = data[data['predictedSpike'] == True].index\n",
    "\n",
    "    data = getSpikeWaveforms(predictedSpikeIndexes, data, window=waveformWindow, signalType=waveformSignalType)\n",
    "\n",
    "    if submission:\n",
    "        print(\"Returning with {} detected spikes.\".format(len(predictedSpikeIndexes)))\n",
    "        return data, predictedSpikeIndexes\n",
    "    else:\n",
    "        data = joinKnownSpikeClasses(data, spikeLocations)\n",
    "        # Assign known labels and drop any detected spikes that refer to more than one label\n",
    "        data, predictedSpikeIndexes = assignKnownClassesToDetectedSpikes(data, predictedSpikeIndexes)\n",
    "\n",
    "        data_training, data_validation, spikeIndexes_training, spikeIndexes_validation = splitData(data, predictedSpikeIndexes)\n",
    "        data=0\n",
    "\n",
    "        print(\"Returning with {} detected spikes.\".format(len(predictedSpikeIndexes)))\n",
    "        return data_training, data_validation, spikeIndexes_training, spikeIndexes_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNeighbour(solution, variation=0.2):\n",
    "    \"\"\"\n",
    "    Function to select next parameter iterations for set of parameters given\n",
    "    :param solution: parameter set\n",
    "    :param variation:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Create 3x1 array of random floats within range [0.0, 1.0)\n",
    "    delta = np.random.random((3, 1))\n",
    "\n",
    "    # Create 3x1 array with each element equal to twice the variation\n",
    "    scale = np.full((3, 1), 2 * variation)\n",
    "\n",
    "    # Create 3x1 array with each element equal to 1 - variation\n",
    "    offset = np.full((3, 1), 1.0 - variation)\n",
    "\n",
    "    # Calculate array of new variation value by multiplying delta and scale arrays and add the offset\n",
    "    a = np.multiply(delta, scale)\n",
    "    a = np.add(a, offset)\n",
    "\n",
    "    newSolution = np.multiply(solution, a.flatten())\n",
    "\n",
    "    return [int(newSolution[0]), int(newSolution[1]), float(newSolution[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getError(supply, df, spikeLocations, demand=99.9):\n",
    "    \"\"\"\n",
    "    Function finds error between target performance and achieved performance of latest solution classification\n",
    "    :param supply: input parameters\n",
    "    :param demand: target performance\n",
    "    :return: return error\n",
    "    \"\"\"\n",
    "    data = df\n",
    "\n",
    "    # Extract next set of parameters\n",
    "    assert isinstance(supply[0], int)\n",
    "    assert isinstance(supply[1], int)\n",
    "    assert isinstance(supply[2], float)\n",
    "\n",
    "    epochs = supply[0]\n",
    "    hidden_nodes = supply[1]\n",
    "    lr = supply[2]\n",
    "\n",
    "    data_training, data_validation, spikeIndexes_training, spikeIndexes_validation = dataPreProcess(data, spikeLocations, waveformWindow=100)\n",
    "\n",
    "    # Train network with new parameters\n",
    "    nn = NeuralNetwork(input_nodes=len(data_training.loc[spikeIndexes_training[0], 'waveform']),\n",
    "                       hidden_nodes=hidden_nodes,\n",
    "                       output_nodes=4,\n",
    "                       lr=lr,\n",
    "                       error_function='difference-squared')\n",
    "\n",
    "    _, _, validationCurve = batchTrain(data_training=data_training,\n",
    "                                       data_validation=data_validation,\n",
    "                                       spikeIndexes_training=spikeIndexes_training,\n",
    "                                       spikeIndexes_validation=spikeIndexes_validation,\n",
    "                                       nn=nn,\n",
    "                                       epochs=epochs,\n",
    "                                       plotCurves=False)\n",
    "\n",
    "    score = validationCurve[-1]\n",
    "\n",
    "    # Return the new error value\n",
    "    return demand - score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acceptanceProbability(oldError, newError, T):\n",
    "    \"\"\"\n",
    "    Calculate the acceptance porbability based on an exponentially decaying relationship to the temperature\n",
    "    :param oldError: \n",
    "    :param newError: \n",
    "    :param T: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    return np.exp((oldError - newError) / T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data\n",
    "demand=99.9\n",
    "solution = [15,500,0.2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spike_tools import dataPreProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to perform simulated annealing.\n",
    ":param df:\n",
    ":param spikeLocations:\n",
    ":param solution:\n",
    ":param alpha:\n",
    ":param iterations:\n",
    ":param demand:\n",
    ":param variation:\n",
    ":param T:\n",
    ":param T_min:\n",
    ":return:\n",
    "\"\"\"\n",
    "\n",
    "x = df\n",
    "\n",
    "# Create new list to store cost values\n",
    "errorValues = []\n",
    "\n",
    "# Generate and append cost of first solution parameters\n",
    "oldError = getError(solution, x, spikeLocations, demand=demand)\n",
    "errorValues.append(oldError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop until temp is below min allowable temp\n",
    "while T > T_min:\n",
    "    i = 1\n",
    "    # Loop until iteration number is above or equal to max allowable number of iterations\n",
    "    while i <= iterations:\n",
    "\n",
    "        # Get new set of solution parameters and generate new cost value using this classification solution\n",
    "        newSolution = getNeighbour(solution, variation=variation)\n",
    "\n",
    "        print(\"It_{}, oldError = {}, newSolution = {}\".format(i, oldError, newSolution))\n",
    "\n",
    "        newError = getError(newSolution, x, spikeLocations, demand=demand)\n",
    "\n",
    "        # Calculate the acceptance probability\n",
    "        pA = __acceptanceProbability(oldError, newError, T)\n",
    "\n",
    "        # If the acceptance probability is above the randomly generated float in the range [0.0,1.0), use the new\n",
    "        # solution as the active solution going forwards and store the cost value\n",
    "        if pA > random.random():\n",
    "            solution = newSolution\n",
    "            oldError = newError\n",
    "\n",
    "        errorValues.append(oldError)\n",
    "        i += 1\n",
    "\n",
    "    # Decay (cool) the temperature and return to the top\n",
    "    T = T * alpha\n",
    "\n",
    "(solution, oldError, errorValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anneal(solution, data, spikeLocations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
