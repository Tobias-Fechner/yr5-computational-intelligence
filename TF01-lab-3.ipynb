{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 Exersizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put some random inputs into an untrained model. How can you explain the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab3.nn_general import NeuralNetwork\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(2,2,1,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsUntrained = nn.queryNN([1,1])\n",
    "resultsUntrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inputs [1,1] were given to an untrained feed-forward, 2-layer NN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is around 0.8. [explain why]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a set of training vectors for all possible inputs, i.e [[0,0], [0,1], [1,0], [1,1]], and a target vector for one of the logical functions such as AND."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [[0,0], [0,1], [1,0], [1,1]]\n",
    "targets = [0,0,0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network once for each target, then query the network for all possible inputs. Has your network successfully learned the AND function? If not, then why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note, the below is wrong, I haven't corrected it yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputVec, target in zip(inputs, targets):\n",
    "    nn.train(inputVec, [target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputVec in inputs:\n",
    "    print(nn.queryNN(inputVec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One iteration of back propagation is not enough. There is no evidence of the network having learned anything. The fourth input, target combination is no closer to an output of 1.\n",
    "\n",
    "Also just realised there are potentially two output nodes when there is only supposed to be one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex 4 - Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extend your training routine to repeat the training process more than once for each target (using a loop), how many iterations doesit take for the network to learn the AND function with two inputs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab3.nn_general import NeuralNetwork\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(2,2,1,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [[0,0], [0,1], [1,0], [1,1]]\n",
    "targets = [0,0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_array = np.array(inputs, ndmin=2).T\n",
    "targets_array = np.array(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USE BELOW FOR DEBUGGING ONLY - REFER TO THE LOOP AFTER THE DIVIDER FOR EXERCISE FUNCTIONALITY. \n",
    "\n",
    "Generate inputs into hidden layer and outputs from hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the inputs list into a 2D array and use to calculate signals into hidden layer\n",
    "hidden_inputs = np.dot(nn.wih, inputs_array)\n",
    "\n",
    "# Calculates the signals emerging from hidden layer\n",
    "hidden_outputs = nn.activation_function(hidden_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate inputs and outputs to/ from final layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the signals into final output layer\n",
    "final_inputs = np.dot(nn.who, hidden_outputs)\n",
    "\n",
    "# Calculate signals emerging from final output layer\n",
    "final_outputs = nn.activation_function(final_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some shape checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final outputs: \", final_outputs)\n",
    "print(\"Weights hidden to output: \", nn.who)\n",
    "print(\"targets array: \", targets_array)\n",
    "print(\"output errors: \", output_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate current error as = target - actual\n",
    "output_errors = targets_array - final_outputs\n",
    "\n",
    "# Hidden layer errors are the output errors, split by the weights, recombined at the hidden nodes\n",
    "hidden_errors = np.dot(nn.who.T, output_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the weights using back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the weights for the links between the hidden and output layers\n",
    "nn.who += nn.lr * np.dot((output_errors * final_outputs * (1.0 - final_outputs)), np.transpose(hidden_outputs))\n",
    "\n",
    "# Update the weights for the links between the input and hidden layers\n",
    "nn.wih += nn.lr * np.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), np.transpose(inputs_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex 4 & 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USE THIS FOR EXERCISE FUNCTIONALITY\n",
    "\n",
    "Train with while loop until below error threshold or after timeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [[0,0], [0,1], [1,0], [1,1]]\n",
    "targets = [0,0,0,1]\n",
    "\n",
    "for lr in np.linspace(0,1,11):\n",
    "    nnEx45 = NeuralNetwork(2,4,1, lr)\n",
    "    bigErrors = np.array([True,True,True,True])\n",
    "    count = 0\n",
    "    \n",
    "    while bigErrors.any() and count < 150000:\n",
    "        errors = nnEx45.train(inputs, targets)\n",
    "        bigErrors = errors > 0.003\n",
    "        count += 1\n",
    "    \n",
    "    print(\"Learning rate: \", lr)\n",
    "    print(\"Count: \", count)\n",
    "    print(\"Errors: \", errors)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn2.queryNN([1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer to question 'How many iterations doesit take for the network to learn the AND function with two inputs?'\n",
    "\n",
    "Trick question! The AND function isn't learned with 2 nodes on the hidden layer. The output for the first three inputs converges to the target (all where target output is zero). But for the final input of [1,1] which should generate an output of [1], the network get's stuck at an error of 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with the number of neurons in the hidden layer, and the learning rate [done above], what effect do these have on training the AND function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the number of nodes in the hidden layer allows us to achieve the target output [1] for the final input combination [1,1]. The minimum number of nodes on the hidden layer required were 4. The LR did not mitigate this blocker in any way. \n",
    "\n",
    "Increasing the LR means the network can be trained to achieve outputs below the error threshold with fewer iterations of the back propagation learning algorithm. Although this didn't happen severely in this case, a too high LR may cause the output error to oscillate. A very minor example of this is seen in the results from LR 0.8-1.0, where a lower final output error was seen for a few of the outputs with lower learning rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have time, try other functions, such as XOR. What effect do the number of neurons in the hidden layer have and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|0     |0     |0     |\n",
    "\n",
    "|0     |1     |1     |\n",
    "\n",
    "|1     |0     |1     |\n",
    "\n",
    "|1     |1     |0     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [[0,0], [0,1], [1,0], [1,1]]\n",
    "targets = [0,1,1,0]\n",
    "\n",
    "for hiddenNodes in [1,2,3,4,5,6,7,8,9,10]:\n",
    "    \n",
    "    nnEx6A = NeuralNetwork(2,hiddenNodes,1, 0.8)\n",
    "    bigErrors = np.array([True,True,True,True])\n",
    "    count = 0\n",
    "    \n",
    "    while bigErrors.any() and count < 150000:\n",
    "        errors = nnEx6A.train(inputs, targets)\n",
    "        bigErrors = errors > 0.003\n",
    "        count += 1\n",
    "\n",
    "    print(\"Nodes in hidden layer: \", hiddenNodes)\n",
    "    print(\"Count: \", count)\n",
    "    print(\"Errors: \", errors)\n",
    "    print(\"Avg. error\", np.mean(errors))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average error has a strange shape when you vary the number of nodes in the hidden layer. Minimum for int range [0 10] at 5 hidden nodes. avg. error then oscillates for the higher number of nodes in hidden layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XNOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|0     |0     |1     |\n",
    "\n",
    "|0     |1     |0     |\n",
    "\n",
    "|1     |0     |0     |\n",
    "\n",
    "|1     |1     |1     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [[0,0], [0,1], [1,0], [1,1]]\n",
    "targets = [1,0,0,1]\n",
    "\n",
    "for hiddenNodes in [1,2,3,4,5,6,7,8,9,10]:\n",
    "    \n",
    "    nnEx6B = NeuralNetwork(2,hiddenNodes,1, 0.8)\n",
    "    bigErrors = np.array([True,True,True,True])\n",
    "    count = 0\n",
    "    \n",
    "    while bigErrors.any() and count < 150000:\n",
    "        errors = nnEx6B.train(inputs, targets)\n",
    "        bigErrors = errors > 0.003\n",
    "        count += 1\n",
    "\n",
    "    print(\"Nodes in hidden layer: \", hiddenNodes)\n",
    "    print(\"Count: \", count)\n",
    "    print(\"Errors: \", errors)\n",
    "    print(\"Avg. error\", np.mean(errors))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lowest avg error again at 5 hidden nodes. XNOR can be implemented too. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do these effects scale if you create, for example, a 4 input logical function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16 combinations for a 4 inputs logic gate\n",
    "inputs = [[0,0,0,0], \n",
    "          [0,0,0,1],\n",
    "          [0,0,1,0],\n",
    "          [0,0,1,1], \n",
    "          [0,1,0,0],\n",
    "          [0,1,0,1],\n",
    "          [0,1,1,0], \n",
    "          [0,1,1,1],\n",
    "          [1,0,0,0],\n",
    "          [1,0,0,1], \n",
    "          [1,0,1,0],\n",
    "          [1,0,1,1],\n",
    "          [1,1,0,0], \n",
    "          [1,1,0,1],\n",
    "          [1,1,1,0],\n",
    "          [1,1,1,1]]\n",
    "# 16 output mappings to the single output node\n",
    "targets = [1]*16\n",
    "\n",
    "for hiddenNodes in [1,2,3,4,5,6,7,8,9,10]:\n",
    "    \n",
    "    nnEx6B = NeuralNetwork(4,hiddenNodes,1, 0.8)\n",
    "    bigErrors = np.array([True,True,True,True])\n",
    "    count = 0\n",
    "    \n",
    "    while bigErrors.any() and count < 150000:\n",
    "        errors = nnEx6B.train(inputs, targets)\n",
    "        bigErrors = errors > 0.003\n",
    "        count += 1\n",
    "\n",
    "    print(\"Nodes in hidden layer: \", hiddenNodes)\n",
    "    print(\"Count: \", count)\n",
    "    print(\"Errors: \", errors)\n",
    "    print(\"Avg. error\", np.mean(errors))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No idea what these results mean tbh..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
